{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "data = pd.read_csv('adco_data_compcrit.csv',encoding='latin-1', header=None)\n",
    "data.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "data= data[['sentiment', 'text']]\n",
    "\n",
    "def relabel_sentiment(label):\n",
    "    if label in [3, 4]:\n",
    "        return 'positive'  # 긍정\n",
    "    elif label == 2:\n",
    "        return 'neutral'   # 중립\n",
    "    elif label in [0, 1]:\n",
    "        return 'negative'  # 부정\n",
    "    else:\n",
    "        return 'unknown'   # 알 수 없는 값\n",
    "\n",
    "data['sentiment'] = data['sentiment'].apply(relabel_sentiment)\n",
    "\n",
    "stopwords = stopwords.words('english') #불용어처리: 관사, 대명사, 전치사, 접속사, 부사...\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "def preprocess_text(text):\n",
    "  tokens = word_tokenize(text.lower()) #소문자로 바꾸고 텍스트 단어 단위로 분리\n",
    "  tokens = [token for token in tokens if token not in stopwords and token not in punctuations] #불용어랑 문장부호 제거\n",
    "  preprocessed_text = ' '.join(tokens) #토큰을 공백으로 연결해 문장형태로 변환 \n",
    "  return preprocessed_text\n",
    "\n",
    "data['text']=data['text'].apply(preprocess_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "model= MultinomialNB() \n",
    "model.fit(X_train_counts, y_train)\n",
    "y_pred = model.predict(X_test_counts)\n",
    "\n",
    "def analyze_sentiments(text, model, vectorizer):\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize #문장분리\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    sentence_vectors = vectorizer.transform(sentences)\n",
    "\n",
    "    predictions = model.predict(sentence_vectors)\n",
    "\n",
    "    total_sentences = len(predictions)\n",
    "    positive_count = sum(1 for sentiment in predictions if sentiment == \"positive\")\n",
    "    negative_count = sum(1 for sentiment in predictions if sentiment == \"negative\")\n",
    "\n",
    "    positive_ratio = positive_count / total_sentences if total_sentences > 0 else 0\n",
    "    negative_ratio = negative_count / total_sentences if total_sentences > 0 else 0\n",
    "\n",
    "    print(f\"칭찬 문장 비율 (긍정): {positive_ratio:.2f}\")\n",
    "    print(f\"비판 문장 비율 (부정): {negative_ratio:.2f}\")\n",
    "    return positive_ratio, negative_ratio"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
