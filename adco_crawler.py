from time import time
start = time()
import requests
from bs4 import BeautifulSoup
import re
# from pykospacing import Spacing
# spacing = Spacing()
# from transformers import pipeline
print(f"[adco_crawler] importing completed : {time() - start:.2} sec")

def remove_emojis(text):
    emoji_pattern = re.compile(
        "[\U00010000-\U0010FFFF" # ìœ ë‹ˆì½”ë“œ ì´ëª¨í‹°ì½˜ ë²”ìœ„
        "\U0001F600-\U0001F64F" # ì›ƒëŠ” ì–¼êµ´ ë° ê°ì •
        "\U0001F300-\U0001F5FF" # ê¸°í˜¸ ë° í”½í† ê·¸ë¨
        "\U0001F680-\U0001F6FF" # êµí†µ ë° ì§€ë„ ê¸°í˜¸
        "\U0001F700-\U0001F77F" # ì•Œì¼€ë¯¸ ê¸°í˜¸
        "\U0001F780-\U0001F7FF" # ê¸°íƒ€ ê¸°í˜¸
        "\U0001F800-\U0001F8FF" # ì¶”ê°€ ê¸°í˜¸
        "\U0001F900-\U0001F9FF" # ì†ë™ì‘ ë° ì¶”ê°€ ì´ëª¨í‹°ì½˜
        "\U0001FA00-\U0001FAFF" # ë„êµ¬ ë° ê°ì²´
        "\U00002702-\U000027B0" # ê¸°í˜¸
        "\U0001F1E6-\U0001F1FF" # êµ­ê¸°
        "\U00002600-\U000026FF" # ì¶”ê°€ ê¸°í˜¸ ë° ê¸°í˜¸ í™•ì¥
        "\U00002700-\U000027BF" # ì¶”ê°€ ê¸°í˜¸ ë° ê¸°í˜¸ í™•ì¥
        "\U0001F550-\U0001F567" # ì‹œê³„ ì´ëª¨í‹°ì½˜
        "\U00002728"            # ë³„: âœ¨
        "\U00002B50"            # ë³„: â­
        "\U00002605-\U00002606" # ë³„: â˜…, â˜†
        "\U0000231A-\U0000231B"   # ì†ëª©ì‹œê³„ (âŒš) ë° ëª¨ë˜ì‹œê³„ (âŒ›)
        "\U0001F570"              # ê¸°íƒ€ ì‹œê³„ ì´ëª¨í‹°ì½˜ (ğŸ•°ï¸)
        "\u200d"                  # Zero Width Joiner (ê¸€ì ì—°ê²° ê¸°í˜¸)
        "\ufe0f"                  # Variation Selector (ì´ëª¨í‹°ì½˜ ë³€í˜•)
        "]+",  # ìœ ë‹ˆì½”ë“œ ì´ëª¨í‹°ì½˜ ë²”ìœ„
        flags=re.UNICODE,
    )   
    return emoji_pattern.sub(r"", text)

    ##emoji_pattern = re.compile(
    ##    "[\U00010000-\U0010FFFF]",  # ìœ ë‹ˆì½”ë“œ ì´ëª¨í‹°ì½˜ ë²”ìœ„
    ##    flags=re.UNICODE,
    ##)
    ##return emoji_pattern.sub(r"", text)

    #"\U000024C2-\U0001F251"  # ê¸°íƒ€

def filter_allowed_characters(text):
    # í—ˆìš© ë²”ìœ„: í•œê¸€, ì˜ë¬¸ì, ìˆ«ì, ê³µë°±, íŠ¹ì • ê¸°í˜¸
    allowed_pattern = re.compile(r"[^a-zA-Z0-9ã„±-ã…ê°€-í£\s!@#$%^&*()\-_=+\[\]{};:'\",.<>/?\\|`~]")
    return allowed_pattern.sub(" ", text)

def classify_food_images(images, classifier):
    food_count = 0
    for idx, img in enumerate(images):
        try:
            # ì´ë¯¸ì§€ srcë¥¼ ê°€ì ¸ì™€ ë‹¤ìš´ë¡œë“œ
            img_url = img['src']
            response = requests.get(img_url, stream=True, timeout=5)
            response.raise_for_status()

            # Transformer ëª¨ë¸ë¡œ ìŒì‹ ì´ë¯¸ì§€ ë¶„ë¥˜
            results = classifier(response.content)
            for result in results:
                if "food" in result["label"].lower():
                    food_count += 1
                    break
        except Exception as e:
            print(f"Error processing image {idx}: {e}")
    return food_count

def fix_spacing(text):
     #1. ë¬¸ì¥ë¶€í˜¸ ë’¤ì— ê³µë°± ì¶”ê°€
    text = re.sub(r"([.,!?])(?=\S)", r"\1 ", text)

     #2. ìˆ«ìì™€ ë¬¸ì ì‚¬ì´ì— ê³µë°± ì¶”ê°€
    text = re.sub(r"(\d)([ê°€-í£a-zA-Z])", r"\1 \2", text)
    text = re.sub(r"([ê°€-í£a-zA-Z])(\d)", r"\1 \2", text)

     #3. ì˜ì–´ ë‹¨ì–´ì™€ í•œê¸€ ì‚¬ì´ì— ê³µë°± ì¶”ê°€
    text = re.sub(r"([a-zA-Z])([ê°€-í£])", r"\1 \2", text)
    text = re.sub(r"([ê°€-í£])([a-zA-Z])", r"\1 \2", text)

     #4. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
    text = re.sub(r"\s+", " ", text).strip()

    return text

def remove_substring(input_string, substring_to_remove):
    return input_string.replace(substring_to_remove, " ")

def crawler(url : str) -> tuple[str]: # ì„ì‹œ í•¨ìˆ˜
    start = time() # debug
    try:
        # URL í™•ì¸
        # if "blog.naver.com" not in url:
        #     return None
        # ì´ì¤‘ ë””ë²„ê¹…ì´ë¼ ì£¼ì„ ì²˜ë¦¬
        
        # HTML ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url)

        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        print(f"[adco_crawler] getting HTML completed : {time() - start:.2} sec")
        start = time()

        # ë‚´ìš© ì¶”ì¶œ
        
        # ì˜ˆì™¸ ì²˜ë¦¬ 1
        iframe = soup.find('iframe', {'id': 'mainFrame'})
        if not (iframe and 'src' in iframe.attrs):
            return None
        print(f"[adco_crawler] exception handling 1 completed : {time() - start:.2} sec")
        start = time()
        
        # ì˜ˆì™¸ ì²˜ë¦¬ 2
        iframe_url = "https://blog.naver.com" + iframe['src']
        iframe_response = requests.get(iframe_url)
        if not (iframe_response.status_code == 200):
            return None
        print(f"[adco_crawler] exception handling 2 completed : {time() - start:.2} sec")
        start = time()
        
        # ì œëª© ì¶”ì¶œ
        iframe_soup = BeautifulSoup(iframe_response.text, 'html.parser')
        content_div = iframe_soup.find('div', {'class': 'se-main-container'})
        title_div = iframe_soup.find('div', {'class': 'se-module se-module-text se-title-text'})
        title = title_div.get_text(strip=True)
        # title = remove_substring(title_div, "\u200b")
        print(f"[adco_crawler] extracting title completed : {time() - start:.2} sec")
        start = time()

        # ì˜ˆì™¸ ì²˜ë¦¬ 3
        if not content_div:
            return None
        print(f"[adco_crawler] exception handling 3 completed : {time() - start:.2} sec")
        start = time()
        
        #ì „ì²´ ë‚´ìš©
        content_div = content_div.get_text(strip=True)
        content_div = remove_substring(content_div, "\u200b")
        content_div = filter_allowed_characters(content_div)
        content_div = fix_spacing(content_div)
        #content_div_text = spacing(content_div)
        #for i in range(0, len(content_div)):
        #    if (content_div[i:i+2] == "  "):
        #        content_div = content_div[:i+1] + content_div[i+2:]
        content = content_div
        print(f"[adco_crawler] extracting text completed : {time() - start:.2} sec")
        start = time()

        # ì´ë¯¸ì§€ ê°¯ìˆ˜
        #images = iframe_soup.find_all('img')
        #classifier = pipeline("image-classification", model="google/vit-base-patch16-224")
        #food_image_count = classify_food_images(images, classifier)
        images = iframe_soup.find_all('img')
        food_image_count = str(len(images))
        print(f"[adco_crawler] counting images completed : {time() - start:.2} sec")
        start = time()

        # tupleë¡œ ë°˜í™˜
        return (content, title, food_image_count)
    
    except Exception as e:
        print(f"Error: {e}")
        return None

if (__name__ == "__main__"):
    
    blog_url = "https://blog.naver.com/cherry_27_/223665563506"
    result = crawler(blog_url)
    if result:
        print(f"Result: {result}")
    else:
        print("Invalid or unsupported Naver blog URL.")

# ì‘ì„±ì : ì´íœ˜ë¯¼
    # ì…ë ¥ : str, ë„¤ì´ë²„ ë¸”ë¡œê·¸ url
    # ë‚´ë¶€ ë™ì‘ : ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ê³¼ ë¸”ë¡œê·¸ ì´ë¦„ë§Œì„ ì¶”ì¶œí•œë‹¤. ì´ë¯¸ì§€ê°€ ì´ ëª‡ê°œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ë„ ê³„ì‚°
    # ì¶œë ¥ : tuple, (ë³¸ë¬¸ ë‚´ìš© : str, ë¸”ë¡œê·¸ ì´ë¦„ : str)

    #return "None" # test value
    #pip install keras==2.11ë¥¼ í„°ë¯¸ë„ì—ì„œ ë‹¤ìš´í•´ì•¼ í•¨ (transformer ìƒí˜¸ ë³´ì™„ì„± ë¬¸ì œ)