{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "from googletrans import Translator\n",
    "from textblob.classifiers import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 갯수 : 17\n",
      "결손 데이터 제거 후 데이터 갯수 : 17\n",
      "0    부모님 모시고 방문했어요 여의도파라곤이라 주차도 편했고 위치도 좋았습니다 룸형태라 ...\n",
      "1    양 정말 푸짐하고 맛있었던 고방채 꼬막비빔밥 새우전 정식 먹었는데 2인분이 3인분같...\n",
      "2    문어 숙회랑 냉수육이 맛있을수있는 음식인지 처음알았어요!! 기대도안했던 어묵탕......\n",
      "dtype: object\n",
      "0    부모님 모시고 방문했어요 여의도파라곤이라 주차도 편했고 위치도 좋았습니다 룸형태라 ...\n",
      "1    양 정말 푸짐하고 맛있었던 고방채 꼬막비빔밥 새우전 정식 먹었는데 인분이 인분같이 ...\n",
      "2    문어 숙회랑 냉수육이 맛있을수있는 음식인지 처음알았어요 기대도안했던 어묵탕맛있는어묵...\n",
      "3    여기는 부모님 모시고 꼭 가보세요 프라이빗한 룸에서 친구들과 편히 수다떨면서 맛난 ...\n",
      "4    아주아주 맛있게 잘 먹었습니다 직장 동료 들과 거하게 먹고 갑니다 역시 고방채 호점...\n",
      "5    가족모임으로 갔는데 너무 좋았어요 룸도 적당히 넓고 깨끗하고 음식도 다 너무 맛있었...\n",
      "6               너무 정갈하고 맛있었습니다  조만간 디너에 친구들과 또 방문하겠습니다\n",
      "7    단독룸이라 회삭 비즈니시미팅 가족 모임하기 좋아요 가마솥 전복죽 먹었는데 전복죽도 ...\n",
      "8             자주 오는 식당이어요 내일도 옵니다 맛있어요 솥밥의 밥이 촉촉하고 맛나요\n",
      "9    여의도를 중심으로 모던한식을 추구하는 고방채  육회와 뭉티기 냉편육모두 보여지는 것...\n",
      "dtype: object\n",
      "리뷰의 최대 길이 : 103\n",
      "리뷰의 평균 길이 : 50.35294117647059\n",
      "[['I', 'visited', 'with', 'my', 'parents.', 'It', 'was', 'Yeouido', 'Paragon,', 'so', 'parking', 'was', 'easy', 'and', 'the', 'location', 'was', 'good.', 'It', 'was', 'a', 'private', 'room,', 'and', 'I', 'was', 'satisfied', 'with', 'the', 'appearance', 'and', 'quality', 'of', 'the', 'food.', 'I', 'was', 'proud', 'to', 'have', 'treated', 'my', 'parents', 'to', 'a', 'delicious', 'meal', 'in', 'a', 'nice', 'place,', 'and', 'they', 'really', 'liked', 'it.', 'Yeouido', 'Korean', 'food.', \"It's\", 'a', 'top', 'home'], ['I', 'had', 'a', 'set', 'meal', 'of', 'Gobangchae', 'cockle', 'bibimbap', 'and', 'shrimp', 'pancake,', 'which', 'was', 'really', 'generous', 'and', 'delicious.', 'The', 'portions', 'were', 'generous', 'for', 'each', 'person,', 'so', 'I', 'think', 'it', 'was', 'even', 'better', 'because', 'I', 'could', 'eat', 'privately', 'in', 'the', 'room.', 'It', 'was', 'a', 'delicious', 'Gobangchae', 'meal', 'in', 'a', 'cozy', 'and', 'comfortable', 'way.'], ['This', 'was', 'the', 'first', 'time', 'I', 'realized', 'that', 'boiled', 'octopus', 'sashimi', 'and', 'cold', 'water', 'meat', 'could', 'be', 'delicious', 'foods.', 'It', 'was', 'my', 'first', 'time', 'eating', 'delicious', 'fish', 'cake', 'soup,', 'which', 'I', \"wasn't\", 'expecting,', 'so', 'I', 'ate', 'almost', 'all', 'of', 'it.', 'The', 'tofu', 'was', 'so', 'savory', 'and', 'so', 'delicious.', 'The', 'meat', 'was', 'a', 'rich', 'red', 'in', 'color,', 'and', 'it', 'was', 'great', 'to', 'eat', 'it', 'together', 'with', 'delicious', 'raw', 'fish', 'for', 'the', 'first', 'time', 'in', 'a', 'while.', 'Seriously,', 'this', 'place', 'tells', 'you', 'why', 'all', 'the', 'menu', 'items', 'are', 'at', 'this', 'price.', 'It', 'was', 'delicious.'], ['Be', 'sure', 'to', 'go', 'here', 'with', 'your', 'parents.', 'I', 'tasted', 'delicious', 'surasang', 'while', 'comfortably', 'chatting', 'with', 'friends', 'in', 'a', 'private', 'room.', 'The', 'staff', 'is', 'friendly', 'and', 'the', 'food', 'is', 'prepared', 'well', 'and', 'is', 'good', 'value', 'for', 'money.', 'This', 'is', 'definitely', 'the', 'place', 'for', 'year-end', 'gatherings.']]\n"
     ]
    }
   ],
   "source": [
    "import textblob.tokenizers\n",
    "\n",
    "\n",
    "with open('datas\\\\review_data.txt', 'r', encoding='UTF-8') as file:\n",
    "    content = file.read()\n",
    "# train_data = content.split('\\n')[3:-1]\n",
    "train_data = content.split('\\n')[3:20]\n",
    "for ite in range(len(train_data)):\n",
    "    train_data[ite] = train_data[ite].split('|')[1]\n",
    "# 결손 데이터 제거\n",
    "print(f\"데이터 갯수 : {len(train_data)}\")\n",
    "temp = train_data.copy()\n",
    "train_data.clear()\n",
    "for ite in range(len(temp)):\n",
    "    if (temp[ite] != ''):\n",
    "        train_data.append(temp[ite])\n",
    "print(f\"결손 데이터 제거 후 데이터 갯수 : {len(train_data)}\")\n",
    "train_data = pd.Series(train_data)\n",
    "print(train_data[:3])\n",
    "train_data = train_data.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "print(train_data[:10])\n",
    "\n",
    "tokenized_data = []\n",
    "tran = Translator()\n",
    "\n",
    "for ko_sent in train_data:\n",
    "    # 토큰화\n",
    "    en_sent = tran.translate(ko_sent, dest = 'en')\n",
    "    en_sent = en_sent.text\n",
    "    tokenized_sentence = en_sent.split(' ')\n",
    "    # stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(tokenized_sentence)\n",
    "\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "print(tokenized_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "training finished : 0.029837846755981445\n",
      "(32, 100)\n",
      "model saved as datas\\w2v_model.bin\n"
     ]
    }
   ],
   "source": [
    "print(\"training start\")\n",
    "start = time.time()\n",
    "model = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
    "print(\"training finished :\", time.time() - start)\n",
    "print(model.wv.vectors.shape)\n",
    "\n",
    "file_index = 0\n",
    "\n",
    "model.save(\"datas\\\\w2v_model.bin\")\n",
    "print(f\"model saved as datas\\\\w2v_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'parking' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ychn0\\anaconda3\\envs\\adco\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ychn0\\anaconda3\\envs\\adco\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'parking' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"parking\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
