{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 갯수 : 3052\n",
      "결손 데이터 제거 후 데이터 갯수 : 2942\n",
      "0    부모님 모시고 방문했어요 여의도파라곤이라 주차도 편했고 위치도 좋았습니다 룸형태라 ...\n",
      "1    양 정말 푸짐하고 맛있었던 고방채 꼬막비빔밥 새우전 정식 먹었는데 2인분이 3인분같...\n",
      "2    문어 숙회랑 냉수육이 맛있을수있는 음식인지 처음알았어요!! 기대도안했던 어묵탕......\n",
      "dtype: object\n",
      "0    부모님 모시고 방문했어요 여의도파라곤이라 주차도 편했고 위치도 좋았습니다 룸형태라 ...\n",
      "1    양 정말 푸짐하고 맛있었던 고방채 꼬막비빔밥 새우전 정식 먹었는데 인분이 인분같이 ...\n",
      "2    문어 숙회랑 냉수육이 맛있을수있는 음식인지 처음알았어요 기대도안했던 어묵탕맛있는어묵...\n",
      "3    여기는 부모님 모시고 꼭 가보세요 프라이빗한 룸에서 친구들과 편히 수다떨면서 맛난 ...\n",
      "4    아주아주 맛있게 잘 먹었습니다 직장 동료 들과 거하게 먹고 갑니다 역시 고방채 호점...\n",
      "5    가족모임으로 갔는데 너무 좋았어요 룸도 적당히 넓고 깨끗하고 음식도 다 너무 맛있었...\n",
      "6               너무 정갈하고 맛있었습니다  조만간 디너에 친구들과 또 방문하겠습니다\n",
      "7    단독룸이라 회삭 비즈니시미팅 가족 모임하기 좋아요 가마솥 전복죽 먹었는데 전복죽도 ...\n",
      "8             자주 오는 식당이어요 내일도 옵니다 맛있어요 솥밥의 밥이 촉촉하고 맛나요\n",
      "9    여의도를 중심으로 모던한식을 추구하는 고방채  육회와 뭉티기 냉편육모두 보여지는 것...\n",
      "dtype: object\n",
      "리뷰의 최대 길이 : 121\n",
      "리뷰의 평균 길이 : 14.405327342747112\n",
      "[['부모님', '모시', '방문', '여의도', '파', '라곤', '이르다', '주차', '편하다', '위치', '좋다', '룸형태', '라', '프라이', '빗', '나오다', '음식', '비주', '얼', '퀄리티', '모두', '만족스럽다', '부모님', '께', '멋지다', '곳', '에서', '맛있다', '식사', '대접', '것', '같다', '뿌듯하다', '부모님', '정말', '좋아하다', '여의도', '한식', '집', '원', '탑'], ['양', '정말', '푸다', '짐', '하고', '맛있다', '방채', '꼬막', '비빔밥', '새우다', '정식', '먹다', '인분', '인분', '같이', '양', '푸다', '짐', '룸', '이라', '프라이', '빗다', '식사', '수', '있다', '더', '좋다', '거', '같다', '아늑하다', '펴다', '식사', '방채', '맛있다', '끼'], ['문어', '숙회', '랑', '냉수', '육이', '맛있다', '음식', '인지', '처음', '알다', '기대다', '어묵', '탕', '맛있다', '어묵', '탕', '처음', '먹다', '보다', '거의', '다', '먹다', '오다', '유부', '고소하다', '왤다', '케', '맛있다', 'ㅠㅠ', '뭉티기', '고기', '색도', '찌다', '빨강', '간만', '맛있다', '육회', '랑', '함께', '먹다', '너무', '좋다', '진심', '모든', '메뉴', '왜', '가격', '인지', '말', '해주다', '곳', '이네', '요', '맛있다'], ['여기다', '부모님', '모시', '꼭', '가보다', '프라이', '빗', '룸', '에서', '친구', '편하다', '수다', '떨다', '맛', '난', '수라상', '맛', '보고오다', '직원', '부들', '친절하다', '먹기', '좋다', '손질', '까지', '가성', '비', '좋다', '연말', '모임', '무조건', '여기다']]\n"
     ]
    }
   ],
   "source": [
    "with open('C:\\\\Users\\\\ychn0\\\\OneDrive\\\\문서\\\\MyProgram\\\\Python\\\\ADCO\\\\datas\\\\review_data.txt', 'r', encoding='UTF-8') as file:\n",
    "    content = file.read()\n",
    "train_data = content.split('\\n')[3:-1]\n",
    "for ite in range(len(train_data)):\n",
    "    train_data[ite] = train_data[ite].split('|')[1]\n",
    "# 결손 데이터 제거\n",
    "print(f\"데이터 갯수 : {len(train_data)}\")\n",
    "temp = train_data.copy()\n",
    "train_data.clear()\n",
    "for ite in range(len(temp)):\n",
    "    if (temp[ite] != ''):\n",
    "        train_data.append(temp[ite])\n",
    "print(f\"결손 데이터 제거 후 데이터 갯수 : {len(train_data)}\")\n",
    "train_data = pd.Series(train_data)\n",
    "print(train_data[:3])\n",
    "train_data = train_data.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "print(train_data[:10])\n",
    "# 불용어 정의\n",
    "josa_file_name = \"datas\\\\adco_data_josa.csv\"\n",
    "with open(josa_file_name, 'r', encoding = \"UTF-8\") as josa_file:\n",
    "    stopwords = josa_file.read().split(',')\n",
    "\n",
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in train_data:\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(stopwords_removed_sentence)\n",
    "with open(\"datas\\\\adco_data_words.csv\",'r',encoding = \"UTF-8\") as file:\n",
    "    guideline_words = file.read().split(',')\n",
    "tokenized_data += guideline_words\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "print(tokenized_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "training finished : 0.31038403511047363\n",
      "(1157, 100)\n",
      "model saved as datas\\w2v_model.bin\n"
     ]
    }
   ],
   "source": [
    "print(\"training start\")\n",
    "start = time.time()\n",
    "model = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)\n",
    "print(\"training finished :\", time.time() - start)\n",
    "print(model.wv.vectors.shape)\n",
    "\n",
    "file_index = 0\n",
    "\n",
    "model.save(\"datas\\\\w2v_model.bin\")\n",
    "print(f\"model saved as datas\\\\w2v_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('좋아하다', 0.9996371269226074), ('많이', 0.9996271133422852), ('않다', 0.9996048808097839), ('시키다', 0.999603271484375), ('소스', 0.9995949864387512), ('제', 0.9995900988578796), ('구이', 0.9995846748352051), ('안', 0.9995818138122559), ('나다', 0.9995814561843872), ('나오다', 0.9995805025100708)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"돈까스\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adco",
   "language": "python",
   "name": "adco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
